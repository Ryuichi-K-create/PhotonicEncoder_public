{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch#\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home_directory = os.path.expanduser('~')\n",
    "onedrive_folder_name = 'OneDrive'\n",
    "onedrive_path = os.path.join(home_directory, onedrive_folder_name)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,)),lambda x: x.view(-1)])\n",
    "\n",
    "root = os.path.join(onedrive_path,'CODES','samples','mnist_data')\n",
    "mnist_train = datasets.MNIST(root=root,download=True,train=True,transform=transform)\n",
    "mnist_test = datasets.MNIST(root=root,download=True,train=False,transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train,batch_size=64,shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_kernels(image, kernel_size):\n",
    "\n",
    "    b,c,h,w = image.shape\n",
    "    kernels = image.unfold(2, kernel_size, kernel_size).unfold(3, kernel_size, kernel_size)\n",
    "    kernels = kernels.contiguous().view(b, -1, kernel_size, kernel_size) #(b,p,k,k)\n",
    "\n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EncoderIM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,kernel_size,leverage,channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.leverage = leverage\n",
    "        kernel_in = int(channels*kernel_size**2)\n",
    "\n",
    "        self.B = nn.Parameter(torch.randn(int(kernel_in/leverage), kernel_in) * (1/np.sqrt(kernel_in)))#.to(device)\n",
    "        self.B.requires_grad = False\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, n_patches, k, _ = x.shape\n",
    "        x = x.view(b*n_patches, -1) \n",
    "        x = x.T\n",
    "        x = torch.matmul(self.B,x)\n",
    "        x = torch.abs(x)**2\n",
    "        x = x.T\n",
    "       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,potential_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        mid_num = int(potential_dim/2)\n",
    "        self.fc1 = nn.Linear(potential_dim, mid_num) \n",
    "        self.fc2 = nn.Linear(mid_num, 10)\n",
    "        self.func = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.func(self.fc1(x))  # 隠れ層\n",
    "        x = self.fc2(x)  # 出力層\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, img_size,channels, kernel_size, leverage):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        kernel_in = kernel_size**2\n",
    "        feat_dim = int(kernel_in/leverage)\n",
    "        num_patches = (28//kernel_size)*(28//kernel_size)\n",
    "        potential_dim = num_patches * feat_dim\n",
    "        self.encoder = Encoder(kernel_size,leverage,channels) \n",
    "        self.bn = nn.BatchNorm1d(feat_dim)#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        self.classifier =  MLP(potential_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b=x.size(0)\n",
    "        x = x.view(b, self.channels, self.img_size, self.img_size)  \n",
    "        x = split_into_kernels(x, self.kernel_size) \n",
    "        x = self.encoder(x) \n",
    "        x = self.bn(x)#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        x = x.reshape(b, -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "img_size = 28\n",
    "leverage = 8\n",
    "kernel_size = 4\n",
    "max_epochs = 10##10\n",
    "\n",
    "num_try = 5##5\n",
    "\n",
    "All_last_loss = []\n",
    "All_loss_test = []\n",
    "All_pro_time = []\n",
    "\n",
    "\n",
    "for num_times in range(num_try): \n",
    "\n",
    "    model = EncoderClassifier(img_size,channels, kernel_size, leverage).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    loss_train_ = []\n",
    "    loss_test_ = []\n",
    "    pro_time_ = []\n",
    "\n",
    "            \n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        sys.stderr.write('\\r%d/%dth Time Epoch: %d/%d' % (num_times+1,num_try, epoch+1, max_epochs)) \n",
    "        sys.stderr.flush()\n",
    "\n",
    "        loss_train = 0\n",
    "        loss_test = 0\n",
    "\n",
    "        start_time1 = time.time()\n",
    "        for (x,t) in train_dataloader:\n",
    "                    \n",
    "            x, t = x.to(device), t.to(device)\n",
    "            y = model(x).to(device)\n",
    "            loss = criterion(y, t) \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "       \n",
    "        loss_train_avg = loss_train / len(train_dataloader)\n",
    "        end_time1 = time.time()\n",
    "        pro_time_.append(end_time1-start_time1)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for x, t in test_dataloader:\n",
    "                x, t = x.to(device), t.to(device)\n",
    "                y = model(x).to(device)\n",
    "                _, predicted = torch.max(y, 1)\n",
    "                loss = criterion(y,t)\n",
    "                loss_test += loss.item()        \n",
    "                total += t.size(0)\n",
    "                correct += (predicted == t).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(t.cpu().numpy())\n",
    "\n",
    "        loss_test_avg = loss_test / len(test_dataloader)\n",
    "\n",
    "        loss_train_.append(loss_train_avg)\n",
    "        loss_test_.append(loss_test_avg)\n",
    "        #if epoch == max_epochs-1:\n",
    "    All_loss_test.append(loss_test_)\n",
    "    All_pro_time.append(sum(pro_time_)) \n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)  # 正規化（行ごとに割合に）\n",
    "\n",
    "    Last_loss_test = loss_test_[-1]\n",
    "    All_last_loss.append(Last_loss_test)\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    print(f\"loss_train: {loss_train_avg:.4f},loss_test: {loss_test_avg:.4f}\")\n",
    "    print(f\"LOSS:{Last_loss_test:.4f}\")\n",
    "    print('ProcessingTime:',sum(pro_time_))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(f\"Overall Correction Rate: {100 * correct / total:.2f}%\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(range(1,len(loss_train_)+1), loss_train_, label=\"Train MSE\", color = 'blue')\n",
    "    ax1.plot(range(1,len(loss_test_)+1), loss_test_, label=\"Test MSE\", color = 'cyan')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('LOSS', color = 'blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.legend()\n",
    "\n",
    "    title = 'LOSS: %dth Time'%(num_times+1) #\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(All_loss_test[0])  # エポック数（各リストの長さ）\n",
    "num_dimensions = len(All_loss_test)  # 埋め込み次元数の数\n",
    "\n",
    "# 各エポックごとに平均と標準偏差を計算\n",
    "mean_loss = np.mean(All_loss_test, axis=0)\n",
    "std_loss = np.std(All_loss_test, axis=0)\n",
    "\n",
    "# グラフの描画\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# 平均値の折れ線グラフと誤差範囲（標準偏差の誤差棒）\n",
    "ax1.errorbar(\n",
    "    x=range(1, epochs + 1), y=mean_loss, yerr=std_loss,\n",
    "    fmt='-o', color='blue', ecolor='blue', capsize=5, \n",
    ")\n",
    "\n",
    "# 軸ラベルやタイトルの設定\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('LOSS', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "plt.title('LOSS Transition in Test data')\n",
    "plt.ylim(0,0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "save_directory1 = os.path.join(onedrive_path,'CODES', 'konishi_Classifer', 'data','texts') \n",
    "print(save_directory1)\n",
    "os.makedirs(save_directory1, exist_ok=True)\n",
    "file_name = 'MLP_IM_Class01_list.csv'##\n",
    "full_path = os.path.join(save_directory1, file_name)\n",
    "with open(full_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    writer.writerow(All_loss_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CENT002",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
