{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bcfe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "-----Formatted time: 8202306 -----\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "sys.path.append(os.path.abspath(\"../..\")) \n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "from dataloader.dataloader import load_MNIST_data,load_CINIC10_data,load_CIFAR10_data,load_Fmnist_data,load_Covtype_data\n",
    "from train.training import train_for_DEQ\n",
    "from train.evaluate import convergence_verify,convergence_verify_tabular,plot_confusion_matrix,plot_loss_curve,plot_errorbar_losscurve,create_table\n",
    "from result_management.data_manager import save_csv,save_experiment_report,create_result_pdf\n",
    "now = datetime.now()\n",
    "formatted_time = now.strftime(\"%m%d%H%M\")\n",
    "formatted_time = int(formatted_time)\n",
    "print(f'-----Formatted time: {formatted_time} -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcbc8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "実験パラメータ報告書を保存しました: /Users/konishi/Library/CloudStorage/OneDrive-個人用(2)/PhotonicEncoder_data/mnist/none_variable/PM/MLP/DEQ8202306/experiment_report.txt\n",
      "----------------------Running with none: 0----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000/1000 completed. Current length: 48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of iterations: 26.761\n",
      "Unresolvable cases: 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/1th Epoch:1/10(2.33%) "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    101\u001b[39m     params_for_train.update({\u001b[33m'\u001b[39m\u001b[33mnum_times\u001b[39m\u001b[33m'\u001b[39m: num_times,\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m: device})\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#-----------training-----------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m loss_train_,loss_test_,pro_time_,Last_loss_test,Test_acc,all_labels,all_preds = \u001b[43mtrain_for_DEQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams_for_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m All_loss_test.append(loss_test_)\n\u001b[32m    107\u001b[39m All_pro_time.append(\u001b[38;5;28msum\u001b[39m(pro_time_))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects_git/PhotnicEncoder/train/training.py:162\u001b[39m, in \u001b[36mtrain_for_DEQ\u001b[39m\u001b[34m(dataset, loss_func, optimizer, lr, num_times, num_try, data_train, data_test, batch_size, device, max_epochs, leverage, enc_type, alpha, cls_type, num_layer, fc, dropout, kernel_size, num_iter, m, tol, beta, gamma, lam)\u001b[39m\n\u001b[32m    160\u001b[39m loss = criterion(y,t)\n\u001b[32m    161\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m optimizer.step()\n\u001b[32m    164\u001b[39m loss_train += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/konishi_res/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/konishi_res/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/konishi_res/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects_git/PhotnicEncoder/models/OtherModels.py:110\u001b[39m, in \u001b[36mDEQFixedPoint.forward.<locals>.backward_hook\u001b[39m\u001b[34m(grad)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_hook\u001b[39m(grad):\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     g_st, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m g_st\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects_git/PhotnicEncoder/models/OtherModels.py:74\u001b[39m, in \u001b[36manderson\u001b[39m\u001b[34m(fc, x0, z_dim, m, num_iter, tol, beta, lam)\u001b[39m\n\u001b[32m     72\u001b[39m X[:, k % m] = beta * (alpha[:, \u001b[38;5;28;01mNone\u001b[39;00m] @ F[:, :n]).squeeze(\u001b[32m1\u001b[39m) + (\u001b[32m1\u001b[39m - beta) * (alpha[:, \u001b[38;5;28;01mNone\u001b[39;00m] @ X[:, :n]).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m     73\u001b[39m x_current = X[:,k%m]\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m F[:, k % m] = \u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m%\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# 残差のノルムを計算（収束判定用）\u001b[39;00m\n\u001b[32m     76\u001b[39m res_norm = (F[:, k % m] - X[:, k % m]).norm().item() / (\u001b[32m1e-5\u001b[39m + F[:, k % m].norm().item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects_git/PhotnicEncoder/models/OtherModels.py:110\u001b[39m, in \u001b[36mDEQFixedPoint.forward.<locals>.backward_hook.<locals>.<lambda>\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_hook\u001b[39m(grad):\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     g_st, _ = \u001b[38;5;28mself\u001b[39m.solver(\u001b[38;5;28;01mlambda\u001b[39;00m y: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m] + grad, grad,z_dim=\u001b[38;5;28mself\u001b[39m.z_dim, **\u001b[38;5;28mself\u001b[39m.kwargs)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m g_st\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/konishi_res/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/konishi_res/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "experiment_type = \"DEQ\"\n",
    "experiment_name = f\"{experiment_type}{formatted_time}\"\n",
    "\n",
    "variable_param = \"none\" #ここで設定した項目は配列にすること(none,leverage,alpha)\n",
    "save = True\n",
    "Show = True\n",
    "\n",
    "params = {\n",
    "    'none':[0], #variable_param=noneの際は1回だけ繰り返す\n",
    "    #data---------------------------------------------\n",
    "    'dataset': 'mnist', # 'mnist', 'cifar-10', 'cinic-10' , 'fashion-mnist'\n",
    "    'batch_size': 100, #64 MNIST, 100 CIFAR10, 100 CINIC10, 512 COVTYPE\n",
    "\n",
    "    #Encoder_Model--------------------------------\n",
    "    'enc_type': 'PM', # 'none', 'MZM', 'LI'\n",
    "    'alpha': np.pi/2, \n",
    "    #位相変調機の感度[np.pi*2,np.pi, np.pi/2, np.pi/4, np.pi/8, np.pi/16],pi:-π~π\n",
    "    #class_model--------------------------------------\n",
    "    'cls_type': 'MLP', # 'MLP' or 'CNN'\n",
    "    'num_layer': 2,\n",
    "    'fc': 'relu',\n",
    "    'dropout': 0.0,\n",
    "\n",
    "    #learning-----------------------------------------\n",
    "    'loss_func': 'cross_entropy',\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.001,\n",
    "\n",
    "    #param--------------------------------------------\n",
    "    'num_try': 1,\n",
    "    'max_epochs': 10,\n",
    "    'leverage': 8, #mnist:[1,2,4,8,16],cinic:[1,2,3,4,6,8,12,16,24,48] enc is not none\n",
    "    'kernel_size': 4,\n",
    "\n",
    "    #anderson param-----------------------------------\n",
    "    'm': 5,\n",
    "    'lam': 1e-5, \n",
    "    'num_iter': 50,\n",
    "    'tol': 1e-4,  #早期終了条件\n",
    "    'beta': 1.0,\n",
    "    'gamma' : 0 #SNLinearRelaxのgamma値(使わない)\n",
    "}\n",
    "#save---------------------------------------------\n",
    "folder_params = {k: params[k] for k in ['dataset', 'enc_type', 'cls_type']}\n",
    "if save:\n",
    "    save_experiment_report(variable_param, params,experiment_name=experiment_name)\n",
    "\n",
    "data_loaders = {\n",
    "    'covtype': load_Covtype_data,\n",
    "    'cifar-10': load_CIFAR10_data,\n",
    "    'cinic-10': load_CINIC10_data,\n",
    "    'mnist': load_MNIST_data,\n",
    "    'fashion-mnist':load_Fmnist_data\n",
    "}\n",
    "convergence_verifies = {\n",
    "    'covtype': convergence_verify_tabular,\n",
    "    'cifar-10': convergence_verify,\n",
    "    'cinic-10': convergence_verify,\n",
    "    'mnist': convergence_verify,\n",
    "    'fashion-mnist': convergence_verify\n",
    "}\n",
    "data_train,data_test = data_loaders[params[\"dataset\"]]()\n",
    "\n",
    "if params[\"enc_type\"] == 'none':\n",
    "    params[\"leverage\"] = 1\n",
    "results = []\n",
    "All_last_LOSSs_ = []\n",
    "All_last_ACCs_ = []\n",
    "All_TIMEs_ = []\n",
    "\n",
    "for variable in params[variable_param]: #variable:leverage,alpha\n",
    "    print(f'----------------------Running with {variable_param}: {variable}----------------------')\n",
    "#-----------------------------------------------------\n",
    "    Relres_ = []\n",
    "    Unresovable = 0\n",
    "    k = 1000\n",
    "    Show_rel = False\n",
    "    for i in range(k):\n",
    "        relres = convergence_verifies[params[\"dataset\"]](params,gamma=params['gamma'],data_train=data_train,data_test=data_test,device=device,Show=Show_rel)\n",
    "        Relres_.append(len(relres))\n",
    "        if len(relres) > 40:\n",
    "            Unresovable += 1\n",
    "        sys.stderr.write(f\"\\rIteration {i+1}/{k} completed. Current length: {len(relres)}\")\n",
    "        sys.stdout.flush()\n",
    "    time.sleep(1)\n",
    "    print(f\"Average number of iterations: {np.mean(Relres_)}\")\n",
    "    print(f\"Unresolvable cases: {Unresovable}\")\n",
    "#-----------------------------------------------------\n",
    "    All_last_loss = []\n",
    "    All_loss_test = []\n",
    "    All_pro_time = []\n",
    "    All_test_acc = []\n",
    "\n",
    "    for num_times in range(params['num_try']):\n",
    "\n",
    "        params_for_train = {k: v for k,v in params.items() if k not in ('none',variable_param)}#配列を除外\n",
    "\n",
    "        if variable_param != 'none': #leverageやalpha可変のとき\n",
    "            params_for_train.update({'num_times': num_times, variable_param: variable,'device': device})\n",
    "        else: #パラメータ不変のとき\n",
    "            params_for_train.update({'num_times': num_times,'device': device})\n",
    "        \n",
    "        #-----------training-----------\n",
    "        loss_train_,loss_test_,pro_time_,Last_loss_test,Test_acc,all_labels,all_preds = train_for_DEQ(**params_for_train,data_train=data_train,data_test=data_test)\n",
    "\n",
    "        All_loss_test.append(loss_test_)\n",
    "        All_pro_time.append(sum(pro_time_))\n",
    "        All_last_loss.append(Last_loss_test)\n",
    "        All_test_acc.append(Test_acc)\n",
    "        if save:\n",
    "            datas = [loss_train_,loss_test_,all_labels,all_preds,Test_acc]\n",
    "            save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='trial',experiment_name=experiment_name)\n",
    "        print(f'Test Accuracy: {Test_acc}')\n",
    "        if Show:\n",
    "            plot_loss_curve(loss_train_,loss_test_,Show=True)\n",
    "            plot_confusion_matrix(all_labels,all_preds,params[\"dataset\"],Test_acc,Show=True)\n",
    "\n",
    "    if save:\n",
    "        datas = [All_loss_test,All_test_acc,All_last_loss,All_pro_time]\n",
    "        save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='mid',experiment_name=experiment_name)\n",
    "\n",
    "        datas = [Relres_,np.mean(Relres_),Unresovable]\n",
    "        save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='relres',experiment_name=experiment_name)\n",
    "    print(f\"Test Accuracy:{Test_acc:.2f}\")\n",
    "    if Show:\n",
    "        plot_errorbar_losscurve(All_loss_test,Show=True)\n",
    "        create_table(All_test_acc,All_last_loss,All_pro_time,Show=True)\n",
    "\n",
    "    All_last_ACCs_.append(All_test_acc)\n",
    "    All_last_LOSSs_.append(All_last_loss)\n",
    "    All_TIMEs_.append(All_pro_time)\n",
    "\n",
    "if variable_param != 'none'and save:\n",
    "    datas = [All_last_ACCs_,All_last_LOSSs_,All_TIMEs_]\n",
    "    save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='final',experiment_name=experiment_name) #最終保存\n",
    "\n",
    "if save:\n",
    "    create_result_pdf(variable_param, params, experiment_name=experiment_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "konishi_res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
