{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc3798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: NVIDIA GeForce GTX 1660\n",
      "Using device: cuda\n",
      "-----Formatted time: 8200926 -----\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "sys.path.append(os.path.abspath(\"../..\")) \n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "from dataloader.dataloader import load_MNIST_data,load_CINIC10_data,load_CIFAR10_data,load_Fmnist_data,load_Covtype_data\n",
    "from train.training import train_for_DEQ, train_nomal\n",
    "from train.evaluate import plot_loss_curve,plot_errorbar_losscurve,plot_confusion_matrix,plot_histograms,create_table,convergence_verify, convergence_verify_tabular\n",
    "from result_management.data_manager import save_csv,auto_git_push,save_experiment_report,create_result_pdf\n",
    "now = datetime.now()\n",
    "formatted_time = now.strftime(\"%m%d%H%M\")\n",
    "formatted_time = int(formatted_time)\n",
    "print(f'-----Formatted time: {formatted_time} -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a11b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "実験パラメータ報告書を保存しました: C:\\Users\\Scent\\OneDrive\\PhotonicEncoder_data\\covtype\\leverage_variable\\PM\\MLP\\DEQ8200926\\experiment_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/3th Epoch:1/50(0.22%) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Running with leverage: 2----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/3th Epoch:1/50(50.88%) "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m     params_for_train\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_times\u001b[39m\u001b[38;5;124m'\u001b[39m: num_times,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device})\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m#-----------training-----------\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m loss_train_,loss_test_,pro_time_,Last_loss_test,Test_acc,all_labels,all_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_for_DEQ\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams_for_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m All_loss_test\u001b[38;5;241m.\u001b[39mappend(loss_test_)\n\u001b[0;32m    100\u001b[0m All_pro_time\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(pro_time_))\n",
      "File \u001b[1;32mc:\\Users\\Scent\\Project\\PhotonicEncoder\\train\\training.py:162\u001b[0m, in \u001b[0;36mtrain_for_DEQ\u001b[1;34m(dataset, loss_func, optimizer, lr, num_times, num_try, data_train, data_test, batch_size, device, max_epochs, leverage, enc_type, alpha, cls_type, num_layer, fc, dropout, kernel_size, num_iter, m, tol, beta, gamma, lam)\u001b[0m\n\u001b[0;32m    160\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y,t)\n\u001b[0;32m    161\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 162\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    164\u001b[0m loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Scent\\anaconda3\\envs\\CENT000\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Scent\\anaconda3\\envs\\CENT000\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_type = \"DEQ\"\n",
    "experiment_name = f\"{experiment_type}{formatted_time}\"\n",
    "\n",
    "variable_param = \"leverage\" #ここで設定した項目は配列にすること(none,leverage,alpha)\n",
    "save = True\n",
    "\n",
    "params = {\n",
    "    'none':[0], #variable_param=noneの際は1回だけ繰り返す\n",
    "    #data---------------------------------------------\n",
    "    'dataset': 'covtype', # 'mnist', 'cifar-10', 'cinic-10' , 'fashion-mnist'\n",
    "    'batch_size': 1024, #64 MNIST, 100 CIFAR10, 100 CINIC10\n",
    "\n",
    "    #Encoder_Model--------------------------------\n",
    "    'enc_type': 'PM', # 'none', 'MZM', 'LI'\n",
    "    'alpha': np.pi/2, \n",
    "    #位相変調機の感度[np.pi*2,np.pi, np.pi/2, np.pi/4, np.pi/8, np.pi/16],pi:-π~π\n",
    "    #class_model--------------------------------------\n",
    "    'cls_type': 'MLP', # 'MLP' or 'CNN'\n",
    "    'num_layer': 3,\n",
    "    'fc': 'relu',\n",
    "    'dropout': 0.0,\n",
    "\n",
    "    #learning-----------------------------------------\n",
    "    'loss_func': 'cross_entropy',\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.001,\n",
    "\n",
    "    #param--------------------------------------------\n",
    "    'num_try': 3,\n",
    "    'max_epochs': 50,\n",
    "    'leverage': [2,6,9,18,27,54], #mnist:[1,2,4,8,16],cinic:[1,2,3,4,6,8,12,16,24,48] enc is not none\n",
    "    'kernel_size': 4,\n",
    "\n",
    "    #anderson param-----------------------------------\n",
    "    'm': 5,\n",
    "    'lam': 1e-5, \n",
    "    'num_iter': 20,\n",
    "    'tol': 1e-4,  #早期終了条件\n",
    "    'beta': 1.0,\n",
    "    'gamma' : 0 #SNLinearRelaxのgamma値(使わない)\n",
    "}\n",
    "#save---------------------------------------------\n",
    "folder_params = {k: params[k] for k in ['dataset', 'enc_type', 'cls_type']}\n",
    "if save:\n",
    "    save_experiment_report(variable_param, params,experiment_name=experiment_name)\n",
    "\n",
    "data_loaders = {\n",
    "    'covtype': load_Covtype_data,\n",
    "    'cifar-10': load_CIFAR10_data,\n",
    "    'cinic-10': load_CINIC10_data,\n",
    "    'mnist': load_MNIST_data,\n",
    "    'fashion-mnist':load_Fmnist_data\n",
    "}\n",
    "\n",
    "data_train,data_test = data_loaders[params[\"dataset\"]]()\n",
    "\n",
    "if params[\"enc_type\"] == 'none':\n",
    "    params[\"leverage\"] = 1\n",
    "results = []\n",
    "All_last_LOSSs_ = []\n",
    "All_last_ACCs_ = []\n",
    "All_TIMEs_ = []\n",
    "\n",
    "for variable in params[variable_param]: #variable:leverage,alpha\n",
    "    print(f'----------------------Running with {variable_param}: {variable}----------------------')\n",
    "#-----------------------------------------------------\n",
    "    Relres_ = []\n",
    "    Unresovable = 0\n",
    "    k = 1000\n",
    "    Show_rel = False\n",
    "    for i in range(k):\n",
    "        relres = convergence_verify_tabular(params,gamma=params['gamma'],data_train=data_train,data_test=data_test,device=device,Show=Show_rel)\n",
    "        Relres_.append(len(relres))\n",
    "        if len(relres) > 18:\n",
    "            Unresovable += 1\n",
    "        sys.stderr.write(f\"\\rIteration {i+1}/{k} completed. Current length: {len(relres)}\")\n",
    "        sys.stdout.flush()\n",
    "    time.sleep(1)\n",
    "    print(f\"Average number of iterations: {np.mean(Relres_)}\")\n",
    "    print(f\"Unresolvable cases: {Unresovable}\")\n",
    "#-----------------------------------------------------\n",
    "    All_last_loss = []\n",
    "    All_loss_test = []\n",
    "    All_pro_time = []\n",
    "    All_test_acc = []\n",
    "\n",
    "    for num_times in range(params['num_try']):\n",
    "\n",
    "        params_for_train = {k: v for k,v in params.items() if k not in ('none',variable_param)}#配列を除外\n",
    "\n",
    "        if variable_param != 'none': #leverageやalpha可変のとき\n",
    "            params_for_train.update({'num_times': num_times, variable_param: variable,'device': device})\n",
    "        else: #パラメータ不変のとき\n",
    "            params_for_train.update({'num_times': num_times,'device': device})\n",
    "        \n",
    "        #-----------training-----------\n",
    "        loss_train_,loss_test_,pro_time_,Last_loss_test,Test_acc,all_labels,all_preds = train_for_DEQ(**params_for_train,data_train=data_train,data_test=data_test)\n",
    "\n",
    "        All_loss_test.append(loss_test_)\n",
    "        All_pro_time.append(sum(pro_time_))\n",
    "        All_last_loss.append(Last_loss_test)\n",
    "        All_test_acc.append(Test_acc)\n",
    "        if save:\n",
    "            datas = [loss_train_,loss_test_,all_labels,all_preds,Test_acc]\n",
    "            save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='trial',experiment_name=experiment_name)\n",
    "        print(f'Test Accuracy: {Test_acc}')\n",
    "        # plot_loss_curve(loss_train_,loss_test_)\n",
    "        # plot_confusion_matrix(all_labels,all_preds,params[\"dataset\"],Test_acc)\n",
    "\n",
    "    if save:\n",
    "        datas = [All_loss_test,All_test_acc,All_last_loss,All_pro_time]\n",
    "        save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='mid',experiment_name=experiment_name)\n",
    "\n",
    "        datas = [Relres_,np.mean(Relres_),Unresovable]\n",
    "        save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='relres',experiment_name=experiment_name)\n",
    "    print(f\"Test Accuracy:{Test_acc:.2f}\")\n",
    "    # plot_errorbar_losscurve(All_loss_test)\n",
    "    # create_table(All_test_acc,All_last_loss,All_pro_time)\n",
    "\n",
    "    All_last_ACCs_.append(All_test_acc)\n",
    "    All_last_LOSSs_.append(All_last_loss)\n",
    "    All_TIMEs_.append(All_pro_time)\n",
    "\n",
    "if variable_param != 'none'and save:\n",
    "    datas = [All_last_ACCs_,All_last_LOSSs_,All_TIMEs_]\n",
    "    save_csv(datas,variable_param,variable,num_times,**folder_params,save_type='final',experiment_name=experiment_name) #最終保存\n",
    "\n",
    "if save:\n",
    "    create_result_pdf(variable_param, params, experiment_name=experiment_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CENT000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
